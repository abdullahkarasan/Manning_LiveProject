{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from tensorflow.keras.metrics import AUC\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dropout, Flatten, Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "#import logging\n",
    "#tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code imports the data you generated before for replicating.\n",
    "\n",
    "stock_prices = pd.read_csv('stock_prices.csv')\n",
    "arima_predictions_T = pd.read_csv('arima_predictions_T')\n",
    "arima_predictions_VZ = pd.read_csv('arima_predictions_VZ')\n",
    "\n",
    "diff_T = stock_prices['T'].diff().dropna()\n",
    "diff_VZ = stock_prices['VZ'].diff().dropna()\n",
    "diff_T = stock_prices['T'].diff().dropna()\n",
    "\n",
    "split = int(len(diff_VZ.values)*0.95)\n",
    "diff_train_T = diff_T.iloc[:split]\n",
    "diff_test_T = diff_T.iloc[split:]\n",
    "\n",
    "split = int(len(diff_VZ.values)*0.95)\n",
    "diff_train_T = diff_T.iloc[:split]\n",
    "diff_test_T = diff_T.iloc[split:]\n",
    "diff_train_VZ = diff_VZ.iloc[:split]\n",
    "diff_test_VZ = diff_VZ.iloc[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10\n",
    "n_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_diff_train_T, y_diff_train_T = split_sequence(diff_train_T, n_steps)\n",
    "X_diff_train_T = X_diff_train_T.reshape((X_diff_train_T.shape[0],\n",
    "                                         X_diff_train_T.shape[1], n_features))\n",
    "\n",
    "X_diff_test_T, y_diff_test_T = split_sequence(diff_test_T.values, n_steps)\n",
    "X_diff_test_T = X_diff_test_T.reshape((X_diff_test_T.shape[0],\n",
    "                                       X_diff_test_T.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_part():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(128,\n",
    "              input_shape=(n_steps, n_features),\n",
    "              return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam' , loss='mean_squared_error', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 1s 210ms/step - loss: 0.3606 - mse: 0.3606 - val_loss: 0.1238 - val_mse: 0.1238\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3195 - mse: 0.3195 - val_loss: 0.1089 - val_mse: 0.1089\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2852 - mse: 0.2852 - val_loss: 0.0838 - val_mse: 0.0838\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2659 - mse: 0.2659 - val_loss: 0.0743 - val_mse: 0.0743\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2412 - mse: 0.2412 - val_loss: 0.0707 - val_mse: 0.0707\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2155 - mse: 0.2155 - val_loss: 0.0702 - val_mse: 0.0702\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1975 - mse: 0.1975 - val_loss: 0.0629 - val_mse: 0.0629\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1801 - mse: 0.1801 - val_loss: 0.0540 - val_mse: 0.0540\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1585 - mse: 0.1585 - val_loss: 0.0463 - val_mse: 0.0463\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1394 - mse: 0.1394 - val_loss: 0.0412 - val_mse: 0.0412\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1336 - mse: 0.1336 - val_loss: 0.0375 - val_mse: 0.0375\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1153 - mse: 0.1153 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0970 - mse: 0.0970 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0913 - mse: 0.0913 - val_loss: 0.0277 - val_mse: 0.0277\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0796 - mse: 0.0796 - val_loss: 0.0242 - val_mse: 0.0242\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0784 - mse: 0.0784 - val_loss: 0.0213 - val_mse: 0.0213\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0622 - mse: 0.0622 - val_loss: 0.0184 - val_mse: 0.0184\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0540 - mse: 0.0540 - val_loss: 0.0162 - val_mse: 0.0162\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0061 - val_mse: 0.0061\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 9.2031e-04 - val_mse: 9.2031e-04\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 7.1591e-04 - val_mse: 7.1591e-04\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 5.2795e-04 - val_mse: 5.2795e-04\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 4.2832e-04 - val_mse: 4.2832e-04\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 2.2006e-04 - val_mse: 2.2006e-04\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 3.5468e-04 - val_mse: 3.5468e-04\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 4.7073e-04 - val_mse: 4.7073e-04\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 3.1928e-04 - val_mse: 3.1928e-04\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 2.0702e-04 - val_mse: 2.0702e-04\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 1.1282e-04 - val_mse: 1.1282e-04\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 1.2416e-04 - val_mse: 1.2416e-04\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 1.1948e-04 - val_mse: 1.1948e-04\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 2.1019e-04 - val_mse: 2.1019e-04\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 2.7393e-04 - val_mse: 2.7393e-04\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 2.6567e-04 - val_mse: 2.6567e-04\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 2.6676e-04 - val_mse: 2.6676e-04\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 1.8589e-04 - val_mse: 1.8589e-04\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 2.5945e-04 - val_mse: 2.5945e-04\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 1.9776e-04 - val_mse: 1.9776e-04\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 1.9089e-04 - val_mse: 1.9089e-04\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 4.0680e-04 - val_mse: 4.0680e-04\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 3.6356e-04 - val_mse: 3.6356e-04\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 1.5871e-04 - val_mse: 1.5871e-04\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 1.6636e-04 - val_mse: 1.6636e-04\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 2.3292e-04 - val_mse: 2.3292e-04\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 1.7079e-04 - val_mse: 1.7079e-04\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 1.8550e-04 - val_mse: 1.8550e-04\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 1.5120e-04 - val_mse: 1.5120e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 1.4389e-04 - val_mse: 1.4389e-04\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 8.9484e-05 - val_mse: 8.9484e-05\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 1.5602e-04 - val_mse: 1.5602e-04\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 2.1320e-04 - val_mse: 2.1320e-04\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 1.7047e-04 - val_mse: 1.7047e-04\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 1.2174e-04 - val_mse: 1.2174e-04\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 1.4056e-04 - val_mse: 1.4056e-04\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 1.8058e-04 - val_mse: 1.8058e-04\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 1.4150e-04 - val_mse: 1.4150e-04\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 9.2249e-05 - val_mse: 9.2249e-05\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 1.1926e-04 - val_mse: 1.1926e-04\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 9.7055e-05 - val_mse: 9.7055e-05\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 1.0702e-04 - val_mse: 1.0702e-04\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 1.6515e-04 - val_mse: 1.6515e-04\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.2235e-04 - val_mse: 1.2235e-04\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 7.8769e-05 - val_mse: 7.8769e-05\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 1.0772e-04 - val_mse: 1.0772e-04\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 6.6868e-05 - val_mse: 6.6868e-05\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 1.3344e-04 - val_mse: 1.3344e-04\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 1.3446e-04 - val_mse: 1.3446e-04\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.0852e-04 - val_mse: 1.0852e-04\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.0044e-04 - val_mse: 1.0044e-04\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 2.2957e-04 - val_mse: 2.2957e-04\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 1.9051e-04 - val_mse: 1.9051e-04\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 2.1687e-04 - val_mse: 2.1687e-04\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 2.5776e-04 - val_mse: 2.5776e-04\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 2.9560e-04 - val_mse: 2.9560e-04\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 3.4392e-04 - val_mse: 3.4392e-04\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 2.2591e-04 - val_mse: 2.2591e-04\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 1.8369e-04 - val_mse: 1.8369e-04\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 3.0395e-04 - val_mse: 3.0395e-04\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 2.2063e-04 - val_mse: 2.2063e-04\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 2.3868e-04 - val_mse: 2.3868e-04\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 2.8644e-04 - val_mse: 2.8644e-04\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 2.3088e-04 - val_mse: 2.3088e-04\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 2.0231e-04 - val_mse: 2.0231e-04\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 1.7765e-04 - val_mse: 1.7765e-04\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 1.6813e-04 - val_mse: 1.6813e-04\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 8.4427e-05 - val_mse: 8.4427e-05\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 1.0144e-04 - val_mse: 1.0144e-04\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 1.8254e-04 - val_mse: 1.8254e-04\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 1.8752e-04 - val_mse: 1.8752e-04\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 1.3375e-04 - val_mse: 1.3375e-04\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 1.8995e-04 - val_mse: 1.8995e-04\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 3.6452e-04 - val_mse: 3.6452e-04\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 2.5535e-04 - val_mse: 2.5535e-04\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 8.3700e-05 - val_mse: 8.3700e-05\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 2.8263e-04 - val_mse: 2.8263e-04\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 3.9998e-04 - val_mse: 3.9998e-04\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 2.4484e-04 - val_mse: 2.4484e-04\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 1.3030e-04 - val_mse: 1.3030e-04\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 1.5888e-04 - val_mse: 1.5888e-04\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.9872e-04 - val_mse: 1.9872e-04\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 1.9940e-04 - val_mse: 1.9940e-04\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 1.9081e-04 - val_mse: 1.9081e-04\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 1.7650e-04 - val_mse: 1.7650e-04\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 2.0089e-04 - val_mse: 2.0089e-04\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.5759e-04 - val_mse: 2.5759e-04\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 2.0773e-04 - val_mse: 2.0773e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 1.6366e-04 - val_mse: 1.6366e-04\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 2.4938e-04 - val_mse: 2.4938e-04\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 1.9413e-04 - val_mse: 1.9413e-04\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 1.2841e-04 - val_mse: 1.2841e-04\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 1.1363e-04 - val_mse: 1.1363e-04\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 1.3523e-04 - val_mse: 1.3523e-04\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 1.4145e-04 - val_mse: 1.4145e-04\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 1.2486e-04 - val_mse: 1.2486e-04\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 1.3760e-04 - val_mse: 1.3760e-04\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 7.3657e-05 - val_mse: 7.3657e-05\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 7.5850e-05 - val_mse: 7.5850e-05\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 1.0007e-04 - val_mse: 1.0007e-04\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 1.3720e-04 - val_mse: 1.3720e-04\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 1.1037e-04 - val_mse: 1.1037e-04\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 9.6238e-05 - val_mse: 9.6238e-05\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 7.7754e-05 - val_mse: 7.7754e-05\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 6.1174e-05 - val_mse: 6.1174e-05\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 8.8290e-05 - val_mse: 8.8290e-05\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 1.6828e-04 - val_mse: 1.6828e-04\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 1.4349e-04 - val_mse: 1.4349e-04\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 7.9121e-05 - val_mse: 7.9121e-05\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 4.9920e-05 - val_mse: 4.9920e-05\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 1.3256e-04 - val_mse: 1.3256e-04\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 1.2085e-04 - val_mse: 1.2085e-04\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 4.9685e-05 - val_mse: 4.9685e-05\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 1.3723e-04 - val_mse: 1.3723e-04\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 2.5970e-04 - val_mse: 2.5970e-04\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 2.3964e-04 - val_mse: 2.3964e-04\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.4454e-04 - val_mse: 1.4454e-04\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.1189e-04 - val_mse: 1.1189e-04\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 8.1728e-05 - val_mse: 8.1728e-05\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 8.5170e-05 - val_mse: 8.5170e-05\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 2.0517e-04 - val_mse: 2.0517e-04\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 1.1532e-04 - val_mse: 1.1532e-04\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 9.7965e-05 - val_mse: 9.7965e-05\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.3223e-04 - val_mse: 1.3223e-04\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 9.1486e-05 - val_mse: 9.1486e-05\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 6.3158e-05 - val_mse: 6.3158e-05\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 1.2274e-04 - val_mse: 1.2274e-04\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 1.1347e-04 - val_mse: 1.1347e-04\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 1.0949e-04 - val_mse: 1.0949e-04\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 1.2904e-04 - val_mse: 1.2904e-04\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 1.5105e-04 - val_mse: 1.5105e-04\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 9.7775e-05 - val_mse: 9.7775e-05\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 1.2800e-04 - val_mse: 1.2800e-04\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 1.1587e-04 - val_mse: 1.1587e-04\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 7.2895e-05 - val_mse: 7.2895e-05\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 8.6110e-05 - val_mse: 8.6110e-05\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 5.5917e-05 - val_mse: 5.5917e-05\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 8.1251e-05 - val_mse: 8.1251e-05\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 9.8523e-05 - val_mse: 9.8523e-05\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 4.4704e-05 - val_mse: 4.4704e-05\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 6.7577e-05 - val_mse: 6.7577e-05\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 9.9313e-05 - val_mse: 9.9313e-05\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 7.0161e-05 - val_mse: 7.0161e-05\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 1.1522e-04 - val_mse: 1.1522e-04\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 1.4871e-04 - val_mse: 1.4871e-04\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 1.2154e-04 - val_mse: 1.2154e-04\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 1.3566e-04 - val_mse: 1.3566e-04\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 1.3305e-04 - val_mse: 1.3305e-04\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 1.1640e-04 - val_mse: 1.1640e-04\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 1.0494e-04 - val_mse: 1.0494e-04\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 7.9790e-05 - val_mse: 7.9790e-05\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 6.1610e-05 - val_mse: 6.1610e-05\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 9.0570e-05 - val_mse: 9.0570e-05\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 1.1824e-04 - val_mse: 1.1824e-04\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 9.9498e-05 - val_mse: 9.9498e-05\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 5.7542e-05 - val_mse: 5.7542e-05\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 4.6212e-05 - val_mse: 4.6212e-05\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 8.3257e-05 - val_mse: 8.3257e-05\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 9.9946e-05 - val_mse: 9.9946e-05\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 6.4270e-05 - val_mse: 6.4270e-05\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 8.5566e-05 - val_mse: 8.5566e-05\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 1.6172e-04 - val_mse: 1.6172e-04\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 1.8428e-04 - val_mse: 1.8428e-04\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 7.7723e-05 - val_mse: 7.7723e-05\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 4.8252e-05 - val_mse: 4.8252e-05\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 9.5684e-05 - val_mse: 9.5684e-05\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 1.0491e-04 - val_mse: 1.0491e-04\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 4.6325e-05 - val_mse: 4.6325e-05\n"
     ]
    }
   ],
   "source": [
    "RNN_model = RNN_part()\n",
    "history_RNN = RNN_model.fit(X_diff_train_T, y_diff_train_T,\n",
    "                            batch_size=200,\n",
    "                            epochs=200,\n",
    "                            validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = X_diff_test_T[X_diff_test_T.shape[0]-1]\n",
    "T_input = start\n",
    "T_input = T_input.reshape((1, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_T = []\n",
    "\n",
    "for i in range(len(arima_predictions_T)):\n",
    "    T_input = T_input.reshape((1, n_steps, n_features))\n",
    "    yhat = RNN_model.predict(T_input, verbose=0)\n",
    "    T_input = np.append(T_input, yhat)\n",
    "    T_input = T_input[1:]\n",
    "    predictions_T.append(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y,yhat):\n",
    "    return np.sqrt(mean_squared_error(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of AT&T for RNN model 0.4337\n"
     ]
    }
   ],
   "source": [
    "print('RMSE of AT&T for RNN model {:.4f}'\\\n",
    "      .format(rmse(diff_test_T, np.array(predictions_T).flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the RMSE score of 0.3254, implying it outperforms the traditional time series models. We know that deep learning model works well with non-linear data. However, please also note that, it is not always the case that deep learning models is superior than the other time series model in terms of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])\n"
     ]
    }
   ],
   "source": [
    "print(history_RNN.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me plot the result of loss and test loss result. It turns out, after huge spike, test loss gets back on track and confirms that RNN performs well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_RNN.history['loss'])\n",
    "plt.plot(history_RNN.history['val_loss'])\n",
    "plt.title('RNN model Loss')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'Test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me run `callback` function to see the effect of early stopping in deep learning. \n",
    "Early stopping is a tool to employ against overfitting in the sense that it helps us to regularize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other pros of early stopping is to save time, because training a deep learning model takes time. In early stopping, once the model reaches the target performance, the algorithm automatically stops. In other word, thanks to `patience` paramater below, training stops after the number of epochs without improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 1s 175ms/step - loss: 0.3569 - mse: 0.3569 - val_loss: 0.1338 - val_mse: 0.1338\n"
     ]
    }
   ],
   "source": [
    "RNN_model = RNN_part()\n",
    "history_RNN = RNN_model.fit(X_diff_train_T, y_diff_train_T,\n",
    "                            batch_size=200,\n",
    "                            epochs=200,\n",
    "                            validation_split=0.2, \n",
    "                            callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_T = []\n",
    "\n",
    "\n",
    "for i in range(len(arima_predictions_T)):\n",
    "    T_input = T_input.reshape((1, n_steps, n_features))\n",
    "    yhat = RNN_model.predict(T_input, verbose=0)\n",
    "    T_input = np.append(T_input, yhat)\n",
    "    T_input = T_input[1:]\n",
    "    predictions_T.append(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of AT&T for RNN model 0.4369\n"
     ]
    }
   ],
   "source": [
    "print('RMSE of AT&T for RNN model {:.4f}'\\\n",
    "      .format(rmse(diff_test_T, np.array(predictions_T).flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_diff_train_VZ, y_diff_train_VZ = split_sequence(diff_train_VZ, n_steps)\n",
    "X_diff_train_VZ = X_diff_train_VZ.reshape((X_diff_train_VZ.shape[0],\n",
    "                                         X_diff_train_VZ.shape[1], n_features))\n",
    "\n",
    "\n",
    "X_diff_test_VZ, y_diff_test_VZ = split_sequence(diff_test_VZ.values, n_steps)\n",
    "X_diff_test_VZ = X_diff_test_VZ.reshape((X_diff_test_VZ.shape[0],\n",
    "                                       X_diff_test_VZ.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_part():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(64,\n",
    "              input_shape=(n_steps, n_features),\n",
    "              return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam' , loss='mean_squared_error', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 1s 171ms/step - loss: 0.7581 - mse: 0.7581 - val_loss: 0.2463 - val_mse: 0.2463\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6428 - mse: 0.6428 - val_loss: 0.2151 - val_mse: 0.2151\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5472 - mse: 0.5472 - val_loss: 0.1914 - val_mse: 0.1914\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5266 - mse: 0.5266 - val_loss: 0.1749 - val_mse: 0.1749\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4573 - mse: 0.4573 - val_loss: 0.1626 - val_mse: 0.1626\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.4205 - mse: 0.4205 - val_loss: 0.1534 - val_mse: 0.1534\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3750 - mse: 0.3750 - val_loss: 0.1475 - val_mse: 0.1475\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3861 - mse: 0.3861 - val_loss: 0.1427 - val_mse: 0.1427\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3482 - mse: 0.3482 - val_loss: 0.1370 - val_mse: 0.1370\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3271 - mse: 0.3271 - val_loss: 0.1306 - val_mse: 0.1306\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3329 - mse: 0.3329 - val_loss: 0.1245 - val_mse: 0.1245\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3218 - mse: 0.3218 - val_loss: 0.1192 - val_mse: 0.1192\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2934 - mse: 0.2934 - val_loss: 0.1132 - val_mse: 0.1132\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2725 - mse: 0.2725 - val_loss: 0.1084 - val_mse: 0.1084\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2499 - mse: 0.2499 - val_loss: 0.1040 - val_mse: 0.1040\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2561 - mse: 0.2561 - val_loss: 0.0989 - val_mse: 0.0989\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2277 - mse: 0.2277 - val_loss: 0.0942 - val_mse: 0.0942\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2366 - mse: 0.2366 - val_loss: 0.0898 - val_mse: 0.0898\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2140 - mse: 0.2140 - val_loss: 0.0839 - val_mse: 0.0839\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1964 - mse: 0.1964 - val_loss: 0.0790 - val_mse: 0.0790\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1790 - mse: 0.1790 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1776 - mse: 0.1776 - val_loss: 0.0693 - val_mse: 0.0693\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1870 - mse: 0.1870 - val_loss: 0.0644 - val_mse: 0.0644\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1578 - mse: 0.1578 - val_loss: 0.0596 - val_mse: 0.0596\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1585 - mse: 0.1585 - val_loss: 0.0548 - val_mse: 0.0548\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1437 - mse: 0.1437 - val_loss: 0.0506 - val_mse: 0.0506\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1297 - mse: 0.1297 - val_loss: 0.0475 - val_mse: 0.0475\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1493 - mse: 0.1493 - val_loss: 0.0442 - val_mse: 0.0442\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1188 - mse: 0.1188 - val_loss: 0.0409 - val_mse: 0.0409\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1079 - mse: 0.1079 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1076 - mse: 0.1076 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1161 - mse: 0.1161 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0930 - mse: 0.0930 - val_loss: 0.0305 - val_mse: 0.0305\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0852 - mse: 0.0852 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0800 - mse: 0.0800 - val_loss: 0.0221 - val_mse: 0.0221\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0782 - mse: 0.0782 - val_loss: 0.0195 - val_mse: 0.0195\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0771 - mse: 0.0771 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0704 - mse: 0.0704 - val_loss: 0.0160 - val_mse: 0.0160\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0540 - mse: 0.0540 - val_loss: 0.0150 - val_mse: 0.0150\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0511 - mse: 0.0511 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0540 - mse: 0.0540 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0470 - mse: 0.0470 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0064 - val_mse: 0.0064\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 8.7347e-04 - val_mse: 8.7347e-04\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 6.7219e-04 - val_mse: 6.7219e-04\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0215 - mse: 0.0215 - val_loss: 5.4953e-04 - val_mse: 5.4953e-04\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0213 - mse: 0.0213 - val_loss: 4.9812e-04 - val_mse: 4.9812e-04\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 7.9299e-04 - val_mse: 7.9299e-04\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 7.2207e-04 - val_mse: 7.2207e-04\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 4.7031e-04 - val_mse: 4.7031e-04\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 3.8400e-04 - val_mse: 3.8400e-04\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 4.8947e-04 - val_mse: 4.8947e-04\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 3.6092e-04 - val_mse: 3.6092e-04\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 3.6224e-04 - val_mse: 3.6224e-04\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 4.5569e-04 - val_mse: 4.5569e-04\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 6.5875e-04 - val_mse: 6.5875e-04\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 5.5814e-04 - val_mse: 5.5814e-04\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 2.9562e-04 - val_mse: 2.9562e-04\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 2.4761e-04 - val_mse: 2.4761e-04\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 2.4350e-04 - val_mse: 2.4350e-04\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 4.2652e-04 - val_mse: 4.2652e-04\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 4.9305e-04 - val_mse: 4.9305e-04\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 4.6412e-04 - val_mse: 4.6412e-04\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 5.5032e-04 - val_mse: 5.5032e-04\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 6.6991e-04 - val_mse: 6.6991e-04\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 6.9561e-04 - val_mse: 6.9561e-04\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 6.2568e-04 - val_mse: 6.2568e-04\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 5.8927e-04 - val_mse: 5.8927e-04\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 5.2455e-04 - val_mse: 5.2455e-04\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 5.1863e-04 - val_mse: 5.1863e-04\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 4.4299e-04 - val_mse: 4.4299e-04\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 5.2184e-04 - val_mse: 5.2184e-04\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 6.4341e-04 - val_mse: 6.4341e-04\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 6.6775e-04 - val_mse: 6.6775e-04\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 5.3767e-04 - val_mse: 5.3767e-04\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 4.1521e-04 - val_mse: 4.1521e-04\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 5.0564e-04 - val_mse: 5.0564e-04\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 7.3780e-04 - val_mse: 7.3780e-04\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 6.0161e-04 - val_mse: 6.0161e-04\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 3.5917e-04 - val_mse: 3.5917e-04\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 4.2425e-04 - val_mse: 4.2425e-04\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 7.0128e-04 - val_mse: 7.0128e-04\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 5.1963e-04 - val_mse: 5.1963e-04\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 3.2600e-04 - val_mse: 3.2600e-04\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 5.0137e-04 - val_mse: 5.0137e-04\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 5.4406e-04 - val_mse: 5.4406e-04\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 3.4526e-04 - val_mse: 3.4526e-04\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 2.5932e-04 - val_mse: 2.5932e-04\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 5.0278e-04 - val_mse: 5.0278e-04\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 6.1524e-04 - val_mse: 6.1524e-04\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 4.8981e-04 - val_mse: 4.8981e-04\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 2.6658e-04 - val_mse: 2.6658e-04\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 2.1881e-04 - val_mse: 2.1881e-04\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 4.6807e-04 - val_mse: 4.6807e-04\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 4.8001e-04 - val_mse: 4.8001e-04\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 4.7300e-04 - val_mse: 4.7300e-04\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 3.7949e-04 - val_mse: 3.7949e-04\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 3.4255e-04 - val_mse: 3.4255e-04\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 5.1089e-04 - val_mse: 5.1089e-04\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 5.8327e-04 - val_mse: 5.8327e-04\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 3.8311e-04 - val_mse: 3.8311e-04\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 3.0773e-04 - val_mse: 3.0773e-04\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 4.8275e-04 - val_mse: 4.8275e-04\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 6.0763e-04 - val_mse: 6.0763e-04\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 7.1461e-04 - val_mse: 7.1461e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 3.7728e-04 - val_mse: 3.7728e-04\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 2.8703e-04 - val_mse: 2.8703e-04\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 4.5330e-04 - val_mse: 4.5330e-04\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 5.6087e-04 - val_mse: 5.6087e-04\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 4.5562e-04 - val_mse: 4.5562e-04\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 3.9882e-04 - val_mse: 3.9882e-04\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 5.2479e-04 - val_mse: 5.2479e-04\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 4.9958e-04 - val_mse: 4.9958e-04\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 4.4239e-04 - val_mse: 4.4239e-04\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 2.7954e-04 - val_mse: 2.7954e-04\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 2.9523e-04 - val_mse: 2.9523e-04\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 4.2564e-04 - val_mse: 4.2564e-04\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 5.4257e-04 - val_mse: 5.4257e-04\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 5.0182e-04 - val_mse: 5.0182e-04\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 4.9769e-04 - val_mse: 4.9769e-04\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 5.3754e-04 - val_mse: 5.3754e-04\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 5.9836e-04 - val_mse: 5.9836e-04\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 5.3368e-04 - val_mse: 5.3368e-04\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 5.6114e-04 - val_mse: 5.6114e-04\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 5.6244e-04 - val_mse: 5.6244e-04\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 5.5621e-04 - val_mse: 5.5621e-04\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 6.6182e-04 - val_mse: 6.6182e-04\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 4.1903e-04 - val_mse: 4.1903e-04\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 2.8529e-04 - val_mse: 2.8529e-04\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 4.6065e-04 - val_mse: 4.6065e-04\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 6.1174e-04 - val_mse: 6.1174e-04\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 4.5624e-04 - val_mse: 4.5624e-04\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 4.8725e-04 - val_mse: 4.8725e-04\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 7.7489e-04 - val_mse: 7.7489e-04\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 6.5541e-04 - val_mse: 6.5541e-04\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 4.0197e-04 - val_mse: 4.0197e-04\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 2.2937e-04 - val_mse: 2.2937e-04\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 3.5293e-04 - val_mse: 3.5293e-04\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 5.6616e-04 - val_mse: 5.6616e-04\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 7.4068e-04 - val_mse: 7.4068e-04\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 6.1146e-04 - val_mse: 6.1146e-04\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 3.9966e-04 - val_mse: 3.9966e-04\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 3.8151e-04 - val_mse: 3.8151e-04\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 3.3713e-04 - val_mse: 3.3713e-04\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 3.4611e-04 - val_mse: 3.4611e-04\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 3.7078e-04 - val_mse: 3.7078e-04\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 2.8797e-04 - val_mse: 2.8797e-04\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 2.0052e-04 - val_mse: 2.0052e-04\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 4.1641e-04 - val_mse: 4.1641e-04\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 5.1885e-04 - val_mse: 5.1885e-04\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 3.2726e-04 - val_mse: 3.2726e-04\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 3.0628e-04 - val_mse: 3.0628e-04\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 3.9292e-04 - val_mse: 3.9292e-04\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 4.7002e-04 - val_mse: 4.7002e-04\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 5.8354e-04 - val_mse: 5.8354e-04\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 5.2542e-04 - val_mse: 5.2542e-04\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 3.0611e-04 - val_mse: 3.0611e-04\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 2.3931e-04 - val_mse: 2.3931e-04\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 3.0939e-04 - val_mse: 3.0939e-04\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 3.4425e-04 - val_mse: 3.4425e-04\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 2.5866e-04 - val_mse: 2.5866e-04\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 2.0423e-04 - val_mse: 2.0423e-04\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 2.9465e-04 - val_mse: 2.9465e-04\n",
      "Epoch 180/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 4.3952e-04 - val_mse: 4.3952e-04\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 5.3012e-04 - val_mse: 5.3012e-04\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 4.0918e-04 - val_mse: 4.0918e-04\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 1.9906e-04 - val_mse: 1.9906e-04\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 2.1023e-04 - val_mse: 2.1023e-04\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 2.8619e-04 - val_mse: 2.8619e-04\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 3.2530e-04 - val_mse: 3.2530e-04\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 3.0483e-04 - val_mse: 3.0483e-04\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 3.1027e-04 - val_mse: 3.1027e-04\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 3.5904e-04 - val_mse: 3.5904e-04\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 4.2137e-04 - val_mse: 4.2137e-04\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 4.8308e-04 - val_mse: 4.8308e-04\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 4.1174e-04 - val_mse: 4.1174e-04\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 3.6293e-04 - val_mse: 3.6293e-04\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 2.8117e-04 - val_mse: 2.8117e-04\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 2.6510e-04 - val_mse: 2.6510e-04\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 2.5479e-04 - val_mse: 2.5479e-04\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 2.7063e-04 - val_mse: 2.7063e-04\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 3.0672e-04 - val_mse: 3.0672e-04\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 3.1135e-04 - val_mse: 3.1135e-04\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 3.2125e-04 - val_mse: 3.2125e-04\n"
     ]
    }
   ],
   "source": [
    "RNN_model = RNN_part()\n",
    "history_RNN_VZ = RNN_model.fit(X_diff_train_VZ, y_diff_train_VZ,\n",
    "                            batch_size=200,\n",
    "                            epochs=200,\n",
    "                            validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = X_diff_test_VZ[X_diff_test_VZ.shape[0]-1]\n",
    "VZ_input = start\n",
    "VZ_input = VZ_input.reshape((1, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_RNN_VZ = []\n",
    "\n",
    "\n",
    "for i in range(len(arima_predictions_VZ)):\n",
    "    VZ_input = VZ_input.reshape((1, n_steps, n_features))\n",
    "    yhat = RNN_model.predict(VZ_input, verbose=0)\n",
    "    VZ_input = np.append(VZ_input, yhat)\n",
    "    VZ_input = VZ_input[1:]\n",
    "    predictions_RNN_VZ.append(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y,yhat):\n",
    "    return np.sqrt(mean_squared_error(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of VZ for RNN model 0.7878\n"
     ]
    }
   ],
   "source": [
    "print('RMSE of VZ for RNN model {:.4f}'\\\n",
    "      .format(rmse(diff_test_VZ, np.array(predictions_RNN_VZ).flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the RMSE score of 0.3254, implying it outperforms the traditional time series models. We know that deep learning model works well with non-linear data. However, please also note that, it is not always the case that deep learning models is superior than the other time series model in terms of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me plot the result of loss and test loss result. It turns out, after huge spike, test loss gets back on track and confirms that RNN performs well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_RNN_VZ.history['loss'])\n",
    "plt.plot(history_RNN_VZ.history['val_loss'])\n",
    "plt.title('RNN model Loss')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'Test'], loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
