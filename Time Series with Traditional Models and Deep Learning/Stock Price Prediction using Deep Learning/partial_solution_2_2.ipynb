{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dropout, Flatten, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code imports the data you generated before for replicating.\n",
    "stock_prices = pd.read_csv('stock_prices.csv')\n",
    "arima_predictions_T = pd.read_csv('arima_predictions_T')\n",
    "diff_T = stock_prices['T'].diff().dropna()\n",
    "split = int(len(diff_T.values)*0.95)\n",
    "diff_train_T = diff_T.iloc[:split]\n",
    "diff_test_T = diff_T.iloc[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10\n",
    "n_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is imported from the previous milestone for replicating.\n",
    "\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is imported from the previous milestone for replicating.\n",
    "\n",
    "X_diff_train_T, y_diff_train_T = split_sequence(diff_train_T, n_steps)\n",
    "X_diff_train_T = X_diff_train_T.reshape((X_diff_train_T.shape[0],\n",
    "                                         X_diff_train_T.shape[1], n_features))\n",
    "\n",
    "X_diff_test_T, y_diff_test_T = split_sequence(diff_test_T.values, n_steps)\n",
    "X_diff_test_T = X_diff_test_T.reshape((X_diff_test_T.shape[0],\n",
    "                                       X_diff_test_T.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is imported from the previous milestone for replicating.\n",
    "\n",
    "def RNN_part():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(128,\n",
    "              input_shape=(n_steps, n_features),\n",
    "              return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam' , loss='mean_squared_error', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "RNN_model = RNN_part()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 1s 200ms/step - loss: 0.3841 - mse: 0.3841 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3277 - mse: 0.3277 - val_loss: 0.1154 - val_mse: 0.1154\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2887 - mse: 0.2887 - val_loss: 0.1022 - val_mse: 0.1022\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2782 - mse: 0.2782 - val_loss: 0.0842 - val_mse: 0.0842\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2373 - mse: 0.2373 - val_loss: 0.0724 - val_mse: 0.0724\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2236 - mse: 0.2236 - val_loss: 0.0633 - val_mse: 0.0633\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2073 - mse: 0.2073 - val_loss: 0.0584 - val_mse: 0.0584\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1852 - mse: 0.1852 - val_loss: 0.0556 - val_mse: 0.0556\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1779 - mse: 0.1779 - val_loss: 0.0546 - val_mse: 0.0546\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1505 - mse: 0.1505 - val_loss: 0.0503 - val_mse: 0.0503\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1431 - mse: 0.1431 - val_loss: 0.0455 - val_mse: 0.0455\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1357 - mse: 0.1357 - val_loss: 0.0402 - val_mse: 0.0402\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1148 - mse: 0.1148 - val_loss: 0.0364 - val_mse: 0.0364\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1156 - mse: 0.1156 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0962 - mse: 0.0962 - val_loss: 0.0289 - val_mse: 0.0289\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0747 - mse: 0.0747 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0673 - mse: 0.0673 - val_loss: 0.0187 - val_mse: 0.0187\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0606 - mse: 0.0606 - val_loss: 0.0162 - val_mse: 0.0162\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0520 - mse: 0.0520 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0451 - mse: 0.0451 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0054 - val_mse: 0.0054\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0158 - mse: 0.0158 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 7.6338e-04 - val_mse: 7.6338e-04\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 6.8484e-04 - val_mse: 6.8484e-04\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 7.1775e-04 - val_mse: 7.1775e-04\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 3.4723e-04 - val_mse: 3.4723e-04\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 2.4901e-04 - val_mse: 2.4901e-04\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 1.6037e-04 - val_mse: 1.6037e-04\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 1.4782e-04 - val_mse: 1.4782e-04\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 1.8428e-04 - val_mse: 1.8428e-04\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 3.0072e-04 - val_mse: 3.0072e-04\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 5.0590e-04 - val_mse: 5.0590e-04\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 2.5129e-04 - val_mse: 2.5129e-04\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 8.1815e-05 - val_mse: 8.1815e-05\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 7.7671e-05 - val_mse: 7.7671e-05\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 1.8964e-04 - val_mse: 1.8964e-04\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 1.8760e-04 - val_mse: 1.8760e-04\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 1.6279e-04 - val_mse: 1.6279e-04\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 1.7255e-04 - val_mse: 1.7255e-04\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 1.4277e-04 - val_mse: 1.4277e-04\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 1.5756e-04 - val_mse: 1.5756e-04\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 1.1807e-04 - val_mse: 1.1807e-04\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 1.1850e-04 - val_mse: 1.1850e-04\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 7.7834e-05 - val_mse: 7.7834e-05\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 1.0277e-04 - val_mse: 1.0277e-04\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 1.6947e-04 - val_mse: 1.6947e-04\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 2.2333e-04 - val_mse: 2.2333e-04\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 2.1097e-04 - val_mse: 2.1097e-04\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 1.1296e-04 - val_mse: 1.1296e-04\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 7.6415e-05 - val_mse: 7.6415e-05\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 9.7878e-05 - val_mse: 9.7878e-05\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 1.2435e-04 - val_mse: 1.2435e-04\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 1.2576e-04 - val_mse: 1.2576e-04\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 1.1879e-04 - val_mse: 1.1879e-04\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 6.4974e-05 - val_mse: 6.4974e-05\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 1.2449e-04 - val_mse: 1.2449e-04\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 3.6633e-04 - val_mse: 3.6633e-04\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 2.6510e-04 - val_mse: 2.6510e-04\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 1.1003e-04 - val_mse: 1.1003e-04\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 2.5976e-04 - val_mse: 2.5976e-04\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 3.0343e-04 - val_mse: 3.0343e-04\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 1.5096e-04 - val_mse: 1.5096e-04\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 7.9838e-05 - val_mse: 7.9838e-05\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 1.4123e-04 - val_mse: 1.4123e-04\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 1.0710e-04 - val_mse: 1.0710e-04\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 1.6491e-04 - val_mse: 1.6491e-04\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 2.1648e-04 - val_mse: 2.1648e-04\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 2.1493e-04 - val_mse: 2.1493e-04\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 2.0640e-04 - val_mse: 2.0640e-04\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 1.3833e-04 - val_mse: 1.3833e-04\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 1.0331e-04 - val_mse: 1.0331e-04\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 1.2713e-04 - val_mse: 1.2713e-04\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 1.7601e-04 - val_mse: 1.7601e-04\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 1.5691e-04 - val_mse: 1.5691e-04\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 1.8007e-04 - val_mse: 1.8007e-04\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 2.0397e-04 - val_mse: 2.0397e-04\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 2.0862e-04 - val_mse: 2.0862e-04\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 2.2109e-04 - val_mse: 2.2109e-04\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.3456e-04 - val_mse: 1.3456e-04\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 8.5449e-05 - val_mse: 8.5449e-05\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 1.9058e-04 - val_mse: 1.9058e-04\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 2.2118e-04 - val_mse: 2.2118e-04\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 1.9640e-04 - val_mse: 1.9640e-04\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 1.9375e-04 - val_mse: 1.9375e-04\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 1.5719e-04 - val_mse: 1.5719e-04\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 1.6253e-04 - val_mse: 1.6253e-04\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 2.5277e-04 - val_mse: 2.5277e-04\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 2.1004e-04 - val_mse: 2.1004e-04\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 9.3606e-05 - val_mse: 9.3606e-05\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 2.2300e-04 - val_mse: 2.2300e-04\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.7120e-04 - val_mse: 1.7120e-04\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 9.5899e-05 - val_mse: 9.5899e-05\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 1.1791e-04 - val_mse: 1.1791e-04\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 1.3077e-04 - val_mse: 1.3077e-04\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.2769e-04 - val_mse: 1.2769e-04\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 1.0962e-04 - val_mse: 1.0962e-04\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 1.0020e-04 - val_mse: 1.0020e-04\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 8.5465e-05 - val_mse: 8.5465e-05\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 7.7031e-05 - val_mse: 7.7031e-05\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 6.4537e-05 - val_mse: 6.4537e-05\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 7.6986e-05 - val_mse: 7.6986e-05\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 5.7777e-05 - val_mse: 5.7777e-05\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 4.8078e-05 - val_mse: 4.8078e-05\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 7.7753e-05 - val_mse: 7.7753e-05\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 1.1792e-04 - val_mse: 1.1792e-04\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 8.6024e-05 - val_mse: 8.6024e-05\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 7.6184e-05 - val_mse: 7.6184e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 1.6322e-04 - val_mse: 1.6322e-04\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 1.4537e-04 - val_mse: 1.4537e-04\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 4.8772e-05 - val_mse: 4.8772e-05\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 6.1393e-05 - val_mse: 6.1393e-05\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 1.0728e-04 - val_mse: 1.0728e-04\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 9.5843e-05 - val_mse: 9.5843e-05\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 1.0131e-04 - val_mse: 1.0131e-04\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 7.0382e-05 - val_mse: 7.0382e-05\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 6.0812e-05 - val_mse: 6.0812e-05\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 7.1358e-05 - val_mse: 7.1358e-05\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 5.8638e-05 - val_mse: 5.8638e-05\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 9.7757e-05 - val_mse: 9.7757e-05\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 1.0944e-04 - val_mse: 1.0944e-04\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 1.0829e-04 - val_mse: 1.0829e-04\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 8.2871e-05 - val_mse: 8.2871e-05\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 7.8137e-05 - val_mse: 7.8137e-05\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 1.4637e-04 - val_mse: 1.4637e-04\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 1.6994e-04 - val_mse: 1.6994e-04\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 1.4320e-04 - val_mse: 1.4320e-04\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 1.9380e-04 - val_mse: 1.9380e-04\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 1.4977e-04 - val_mse: 1.4977e-04\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 1.5023e-04 - val_mse: 1.5023e-04\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.3041e-04 - val_mse: 2.3041e-04\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 2.5565e-04 - val_mse: 2.5565e-04\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 1.5729e-04 - val_mse: 1.5729e-04\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 1.2075e-04 - val_mse: 1.2075e-04\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 2.0003e-04 - val_mse: 2.0003e-04\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 1.5431e-04 - val_mse: 1.5431e-04\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 1.4691e-04 - val_mse: 1.4691e-04\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 2.0924e-04 - val_mse: 2.0924e-04\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 1.5377e-04 - val_mse: 1.5377e-04\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 8.3877e-05 - val_mse: 8.3877e-05\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 9.3814e-05 - val_mse: 9.3814e-05\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 8.9697e-05 - val_mse: 8.9697e-05\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 7.5285e-05 - val_mse: 7.5285e-05\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 8.8832e-05 - val_mse: 8.8832e-05\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 1.4182e-04 - val_mse: 1.4182e-04\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 1.4992e-04 - val_mse: 1.4992e-04\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 9.7551e-05 - val_mse: 9.7551e-05\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 8.5650e-05 - val_mse: 8.5650e-05\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 7.6090e-05 - val_mse: 7.6090e-05\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 7.6297e-05 - val_mse: 7.6297e-05\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 1.2521e-04 - val_mse: 1.2521e-04\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 1.1713e-04 - val_mse: 1.1713e-04\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 5.3942e-05 - val_mse: 5.3942e-05\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 5.9344e-05 - val_mse: 5.9344e-05\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 7.4902e-05 - val_mse: 7.4902e-05\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 6.7627e-05 - val_mse: 6.7627e-05\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 6.6373e-05 - val_mse: 6.6373e-05\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 6.2843e-05 - val_mse: 6.2843e-05\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 8.6439e-05 - val_mse: 8.6439e-05\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 8.7223e-05 - val_mse: 8.7223e-05\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 8.0332e-05 - val_mse: 8.0332e-05\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 1.3096e-04 - val_mse: 1.3096e-04\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 9.9150e-05 - val_mse: 9.9150e-05\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 6.4327e-05 - val_mse: 6.4327e-05\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 6.2506e-05 - val_mse: 6.2506e-05\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 6.6345e-05 - val_mse: 6.6345e-05\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 5.3026e-05 - val_mse: 5.3026e-05\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 4.5863e-05 - val_mse: 4.5863e-05\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 6.2928e-05 - val_mse: 6.2928e-05\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 6.2946e-05 - val_mse: 6.2946e-05\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 6.7780e-05 - val_mse: 6.7780e-05\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 9.2147e-05 - val_mse: 9.2147e-05\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 1.0780e-04 - val_mse: 1.0780e-04\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 1.4593e-04 - val_mse: 1.4593e-04\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 1.5088e-04 - val_mse: 1.5088e-04\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 7.3180e-05 - val_mse: 7.3180e-05\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 9.5276e-05 - val_mse: 9.5276e-05\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 1.4644e-04 - val_mse: 1.4644e-04\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 1.0257e-04 - val_mse: 1.0257e-04\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 5.5537e-05 - val_mse: 5.5537e-05\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 8.6642e-05 - val_mse: 8.6642e-05\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 1.1703e-04 - val_mse: 1.1703e-04\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 8.5999e-05 - val_mse: 8.5999e-05\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 7.2055e-05 - val_mse: 7.2055e-05\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 6.3770e-05 - val_mse: 6.3770e-05\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 6.3915e-05 - val_mse: 6.3915e-05\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 9.5795e-05 - val_mse: 9.5795e-05\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 1.1305e-04 - val_mse: 1.1305e-04\n"
     ]
    }
   ],
   "source": [
    "RNN_model = RNN_part()\n",
    "history_RNN = RNN_model.fit(X_diff_train_T, y_diff_train_T,\n",
    "                            batch_size=200,\n",
    "                            epochs=200,\n",
    "                            validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is imported from the previous milestone for replicating.\n",
    "\n",
    "start = X_diff_test_T[X_diff_test_T.shape[0]-1]\n",
    "T_input = start\n",
    "T_input = T_input.reshape((1, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is imported from the previous milestone for replicating.\n",
    "\n",
    "\n",
    "predictions_T = []\n",
    "\n",
    "for i in range(len(arima_predictions_T)):\n",
    "    T_input = T_input.reshape((1, n_steps, n_features))\n",
    "    yhat = RNN_model.predict(T_input, verbose=0)\n",
    "    T_input = np.append(T_input, yhat)\n",
    "    T_input = T_input[1:]\n",
    "    predictions_T.append(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y,yhat):\n",
    "    return np.sqrt(mean_squared_error(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of AT&T for RNN model 0.4785\n"
     ]
    }
   ],
   "source": [
    "print('RMSE of AT&T for RNN model {:.4f}'\\\n",
    "      .format(rmse(diff_test_T, np.array(predictions_T).flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the RMSE score of 0.3254, implying it outperforms the traditional time series models. We know that deep learning model works well with non-linear data. However, please also note that, it is not always the case that deep learning models is superior than the other time series model in terms of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])\n"
     ]
    }
   ],
   "source": [
    "print(history_RNN.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me plot the result of loss and test loss result. It turns out, after huge spike, test loss gets back on track and confirms that RNN performs well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_RNN.history['loss'])\n",
    "plt.plot(history_RNN.history['val_loss'])\n",
    "plt.title('RNN model Loss')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'Test'], loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
